# -*- coding: utf-8 -*-
"""IT_Purplehack_Alphabank_uplift predict

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bQgltXRDLetNUVgSgZmK_LoY-YtEc0jP

# 1. Baseline
"""

import numpy as np
import pandas as pd
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split

from sklearn.impute import SimpleImputer

!pip freeze | grep "numpy\|pandas\|lightgbm\|scikit-learn"

"""## Загрузка данных"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My Drive/input data

train_df = pd.read_parquet("train_data.pqt")
test_df = pd.read_parquet("test_data.pqt")

cat_cols = [
    "channel_code", "city", "city_type",
    "okved", "segment", "start_cluster",
    "index_city_code", "ogrn_month", "ogrn_year"
]

"""Обозначение категориальных признаков"""

train_df[cat_cols] = train_df[cat_cols].astype("category")
test_df[cat_cols] = test_df[cat_cols].astype("category")



"""Создаем выборки для валидации и обучения"""

X = train_df.drop(["id", "date", "end_cluster"], axis=1)
y = train_df["end_cluster"]

x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

"""НАШ КОД

Заполним m6
"""

m1 = train_df[train_df['date']=='month_1']
m2 = train_df[train_df['date']=='month_2']
m3 = train_df[train_df['date']=='month_3']

m1 = m1[['id','start_cluster']]
m2 = m2[['id','start_cluster']]
m3 = m3[['id','start_cluster']]

m1_2 = pd.merge(m1, m2, on='id', how='left')
df = pd.merge(m1_2, m3, on='id', how='left')

df = df.set_index(['id'])

X = df[['start_cluster_x', 'start_cluster_y']]
y = df["start_cluster"]

x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

model_m6 = LGBMClassifier(verbosity=-1, random_state=42, n_jobs=-1)
model_m6.fit(x_train, y_train)

m4 = test_df[test_df['date']=='month_4']
m5 = test_df[test_df['date']=='month_5']
m6 = test_df[test_df['date']=='month_6']

m4 = m4[['id','start_cluster']]
m5 = m5[['id','start_cluster']]
m6 = m6[['id','start_cluster']]

m4_5 = pd.merge(m4, m5, on='id', how='left')
df2 = pd.merge(m4_5, m6, on='id', how='left')

df_test = m4_5.set_index('id')

test_pred_proba = model_m6.predict_proba(df_test)

test_pred_proba_df = pd.DataFrame(test_pred_proba, columns= model_m6.classes_)
sorted_classes = sorted(test_pred_proba_df.columns.to_list())
test_pred_proba_df = test_pred_proba_df[sorted_classes]

res = test_pred_proba_df.idxmax(axis=1)
res = pd.DataFrame(res)

df2['start_cluster'] = np.where(df2['start_cluster'] == 'NaN', res[0], res[0])

df2 = df2.drop(columns=['start_cluster_x','start_cluster_y'], axis=1)

df2.head(2) #стартоый кластер для месяца 6

df_m6 = test_df[test_df['date'] =='month_6']

df_m6.shape

df_proba = pd.merge(df_m6, df2, on='id', how='left')
df_proba = df_proba.drop(columns=['start_cluster_x'], axis=1)
df_proba = df_proba.rename(columns = {'start_cluster_y':'start_cluster'})

m45 = test_df [(test_df['date'] =='month_4') |  (test_df['date'] =='month_5')]

combine = pd.concat([m45, df_proba])
combine = combine.sort_values(['id'])
#combine['start_cluster'] = np.where(combine['start_cluster'].isnull(), combine['start_cluster_y'],combine['start_cluster'])
#combine = combine.drop(columns=['start_cluster_y'], axis=1)
combine.head(3)

combine.to_csv("combine.csv", index=False)

test_df.shape #нужно предсказать

last_m_test_df = combine[combine["date"] == "month_6"]
last_m_test_df = last_m_test_df.drop(["id", "date"], axis=1)

last_m_test_df["start_cluster"] = last_m_test_df["start_cluster"].astype("category")

"""заполняем пропуски mean


"""

cat_cols = [
    "channel_code", "city", "city_type",
    "okved", "segment", "start_cluster",
    "index_city_code", "ogrn_month", "ogrn_year"
]

train_df["start_cluster"] = train_df["start_cluster"].mode()[0]
train_df["channel_code"] = train_df["channel_code"].mode()[0]
train_df["index_city_code"] = train_df["index_city_code"].mode()[0]
train_df["city"] = train_df["city"].mode()[0]
train_df["segment"] = train_df["segment"].mode()[0]
train_df["ogrn_month"] = train_df["ogrn_month"].mode()[0]

imp= SimpleImputer(strategy='most_frequent')

df = pd.DataFrame(
     imp.fit_transform(train_df), columns=train_df.columns
).astype(train_df.dtypes.to_dict())

X = df.drop(["id", "date", "end_cluster"], axis=1)
y = df["end_cluster"]

x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

subset1 = combine[['channel_code', 'city', 'city_type', 'index_city_code']]
subset2 = combine[['balance_amt_avg', 'balance_amt_max', 'balance_amt_min','balance_amt_day_avg']]
subset3 = combine[[ 'ogrn_days_end_month', 'ogrn_days_end_quarter', 'ogrn_month', 'ogrn_year', 'ft_registration_date', 'max_founderpres', 'min_founderpres', 'ogrn_exist_months', 'okved', 'segment']]
subset4 = combine[['sum_of_paym_2m', 'sum_of_paym_6m', 'sum_of_paym_1y']]
subset5 = combine[['sum_a_oper_1m', 'cnt_a_oper_1m']]
subset6 = combine[['sum_b_oper_1m', 'cnt_b_oper_1m']]
subset7 = combine[['sum_c_oper_1m', 'cnt_c_oper_1m']]
subset8 = combine[['sum_deb_d_oper_1m', 'cnt_deb_d_oper_1m', 'sum_cred_d_oper_1m', 'cnt_cred_d_oper_1m']]
subset9 = combine[['sum_deb_e_oper_1m', 'cnt_deb_e_oper_1m', 'cnt_days_deb_e_oper_1m', 'sum_cred_e_oper_1m', 'cnt_cred_e_oper_1m', 'cnt_days_cred_e_oper_1m',]]
subset10 = combine[['sum_deb_f_oper_1m', 'cnt_deb_f_oper_1m', 'cnt_days_deb_f_oper_1m', 'sum_cred_f_oper_1m', 'cnt_cred_f_oper_1m', 'cnt_days_cred_f_oper_1m']] #нельзя
subset11 = combine[['sum_deb_g_oper_1m', 'cnt_deb_g_oper_1m', 'cnt_days_deb_g_oper_1m', 'sum_cred_g_oper_1m', 'cnt_cred_g_oper_1m', 'cnt_days_cred_g_oper_1m']]
subset12 = combine[['sum_deb_h_oper_1m', 'cnt_deb_h_oper_1m', 'cnt_days_deb_h_oper_1m', 'sum_cred_h_oper_1m', 'cnt_cred_h_oper_1m', 'cnt_days_cred_h_oper_1m']]
subset13 =  combine[['sum_a_oper_3m', 'cnt_a_oper_3m']]
subset14 =  combine[['sum_b_oper_3m', 'cnt_b_oper_3m']]
subset15 =  combine[['sum_c_oper_3m', 'cnt_c_oper_3m']]
subset16 =  combine[['sum_deb_d_oper_3m', 'cnt_deb_d_oper_3m', 'sum_cred_d_oper_3m', 'cnt_cred_d_oper_3m']]
subset17 =  combine[[ 'sum_deb_e_oper_3m', 'cnt_deb_e_oper_3m', 'cnt_days_deb_e_oper_3m', 'sum_cred_e_oper_3m', 'cnt_cred_e_oper_3m', 'cnt_days_cred_e_oper_3m']]
subset18 =   combine[['sum_deb_f_oper_3m', 'cnt_deb_f_oper_3m', 'cnt_days_deb_f_oper_3m', 'sum_cred_f_oper_3m', 'cnt_cred_f_oper_3m', 'cnt_days_cred_f_oper_3m']]
subset19 =  combine[['sum_deb_g_oper_3m', 'cnt_deb_g_oper_3m', 'cnt_days_deb_g_oper_3m', 'sum_cred_g_oper_3m', 'cnt_cred_g_oper_3m', 'cnt_days_cred_g_oper_3m']]
subset20 =  combine[['sum_deb_h_oper_3m', 'cnt_deb_h_oper_3m', 'cnt_days_deb_h_oper_3m', 'sum_cred_h_oper_3m', 'cnt_cred_h_oper_3m', 'cnt_days_cred_h_oper_3m']]

"""## Обучение модели

В качестве базовой модели возьмем LGBM обучим на всех признаках
"""

model = LGBMClassifier(verbosity=-1, random_state=42, n_jobs=-1)
model.fit(x_train, y_train)

def weighted_roc_auc(y_true, y_pred, labels, weights_dict):
    unnorm_weights = np.array([weights_dict[label] for label in labels])
    weights = unnorm_weights / unnorm_weights.sum()
    classes_roc_auc = roc_auc_score(y_true, y_pred, labels=labels,
                                    multi_class="ovr", average=None)
    return sum(weights * classes_roc_auc)

cluster_weights = pd.read_excel("cluster_weights.xlsx").set_index("cluster")
weights_dict = cluster_weights["unnorm_weight"].to_dict()

#model.feature_importances_

"""Зададим функцию для взвешенной метрики roc auc

Проверка работы модели
"""

y_pred_proba = model.predict_proba(x_val)
y_pred_proba.shape

weighted_roc_auc(y_val, y_pred_proba, model.classes_, weights_dict)

"""ЛИН РЕГ

"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold

# selecting the classifier
log_reg= LogisticRegression()

# fitting the model on resampled data
log_reg.fit(x_train, y_train)

# printing best score and best parameters
#print("best score is:" ,gridsearch_log.best_score_)
#print("best parameters are:" ,gridsearch_log.best_params_)

"""## Прогноз на тестовой выборке"""

test_df.pivot(index="id", columns="date", values="start_cluster").head(3)

"""Для того, чтобы сделать прогноз на тестовой выборке, нужно заполнить стартовый кластер. </br>
В качестве базового подхода заполним все стартовые кластеры, самым популярным кластером.
"""

test_df["start_cluster"] = train_df["start_cluster"].mode()[0]
test_df["start_cluster"] = test_df["start_cluster"].astype("category")

sample_submission_df = pd.read_csv("sample_submission.csv")

sample_submission_df.shape

sample_submission_df.head()

"""Для тестовой выборки будем использовать только последний месяц"""

last_m_test_df = test_df[test_df["date"] == "month_6"]
last_m_test_df = last_m_test_df.drop(["id", "date"], axis=1)

test_pred_proba = model.predict_proba(last_m_test_df)
test_pred_proba_df = pd.DataFrame(test_pred_proba, columns=model.classes_)
sorted_classes = sorted(test_pred_proba_df.columns.to_list())
test_pred_proba_df = test_pred_proba_df[sorted_classes]

test_pred_proba_df.shape

test_pred_proba_df.head(2)

sample_submission_df[sorted_classes] = test_pred_proba_df
sample_submission_df.to_csv("baseline_submission_attempt1.csv", index=False)